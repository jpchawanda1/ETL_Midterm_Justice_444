{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fcfd91f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed files saved to 'transformed/transformed_full.csv' and 'transformed/transformed_incremental.csv'\n",
      "\n",
      "=== Head of transformed_full.csv ===\n",
      "+----+------------+-----------------+-----------+-----------+------------+--------------+---------------+--------------+----------+\n",
      "|    |   order_id | customer_name   | product   | Purpose   |   quantity |   unit_price |   total_price | order_date   | region   |\n",
      "+====+============+=================+===========+===========+============+==============+===============+==============+==========+\n",
      "|  0 |          1 | Diana           | Tablet    | Other     |          2 |          500 |          1000 | 2024-01-20   | South    |\n",
      "+----+------------+-----------------+-----------+-----------+------------+--------------+---------------+--------------+----------+\n",
      "|  1 |          2 | Eve             | Laptop    | Leisure   |          2 |          500 |          1000 | 2024-04-29   | North    |\n",
      "+----+------------+-----------------+-----------+-----------+------------+--------------+---------------+--------------+----------+\n",
      "|  2 |          3 | Charlie         | Laptop    | Other     |          2 |          250 |           500 | 2024-01-08   | Unknown  |\n",
      "+----+------------+-----------------+-----------+-----------+------------+--------------+---------------+--------------+----------+\n",
      "|  3 |          4 | Eve             | Laptop    | Other     |          2 |          750 |          1500 | 2024-01-07   | West     |\n",
      "+----+------------+-----------------+-----------+-----------+------------+--------------+---------------+--------------+----------+\n",
      "|  4 |          5 | Eve             | Tablet    | Work      |          3 |          500 |          1500 | 2024-03-07   | South    |\n",
      "+----+------------+-----------------+-----------+-----------+------------+--------------+---------------+--------------+----------+\n",
      "\n",
      "=== Head of transformed_incremental.csv ===\n",
      "+----+------------+-----------------+-----------+-----------+------------+--------------+---------------+--------------+----------+\n",
      "|    |   order_id | customer_name   | product   | Purpose   |   quantity |   unit_price |   total_price | order_date   | region   |\n",
      "+====+============+=================+===========+===========+============+==============+===============+==============+==========+\n",
      "|  0 |        101 | Alice           | Laptop    | Leisure   |          2 |          900 |          1800 | 2024-05-09   | Central  |\n",
      "+----+------------+-----------------+-----------+-----------+------------+--------------+---------------+--------------+----------+\n",
      "|  1 |        102 | Unknown         | Laptop    | Leisure   |          1 |          300 |           300 | 2024-05-07   | Central  |\n",
      "+----+------------+-----------------+-----------+-----------+------------+--------------+---------------+--------------+----------+\n",
      "|  2 |        103 | Unknown         | Laptop    | Leisure   |          1 |          600 |           600 | 2024-05-04   | Central  |\n",
      "+----+------------+-----------------+-----------+-----------+------------+--------------+---------------+--------------+----------+\n",
      "|  3 |        104 | Unknown         | Tablet    | Work      |          2 |          300 |           600 | 2024-05-26   | Central  |\n",
      "+----+------------+-----------------+-----------+-----------+------------+--------------+---------------+--------------+----------+\n",
      "|  4 |        105 | Heidi           | Tablet    | Other     |          2 |          600 |          1200 | 2024-05-21   | North    |\n",
      "+----+------------+-----------------+-----------+-----------+------------+--------------+---------------+--------------+----------+\n",
      "\n",
      "=== Transformation Summary: transformed_full.csv ===\n",
      "+-----------------------+---------------------------------+\n",
      "| File                  | transformed_full.csv            |\n",
      "+-----------------------+---------------------------------+\n",
      "| Rows                  | 99                              |\n",
      "+-----------------------+---------------------------------+\n",
      "| Duplicate Rows        | 0                               |\n",
      "+-----------------------+---------------------------------+\n",
      "| Any Missing Values    | False                           |\n",
      "+-----------------------+---------------------------------+\n",
      "| Purpose Unique Values | Other, Leisure, Work, Education |\n",
      "+-----------------------+---------------------------------+\n",
      "\n",
      "=== Transformation Summary: transformed_incremental.csv ===\n",
      "+-----------------------+-----------------------------+\n",
      "| File                  | transformed_incremental.csv |\n",
      "+-----------------------+-----------------------------+\n",
      "| Rows                  | 10                          |\n",
      "+-----------------------+-----------------------------+\n",
      "| Duplicate Rows        | 0                           |\n",
      "+-----------------------+-----------------------------+\n",
      "| Any Missing Values    | False                       |\n",
      "+-----------------------+-----------------------------+\n",
      "| Purpose Unique Values | Leisure, Work, Other        |\n",
      "+-----------------------+-----------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpcha\\AppData\\Local\\Temp\\ipykernel_19508\\4017435703.py:70: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.loc[:, col] = df[col].fillna(pd.Timestamp('1970-01-01'))\n",
      "C:\\Users\\jpcha\\AppData\\Local\\Temp\\ipykernel_19508\\4017435703.py:70: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.loc[:, col] = df[col].fillna(pd.Timestamp('1970-01-01'))\n"
     ]
    }
   ],
   "source": [
    "# --- ETL Transform Step ---\n",
    "# Apply at least 4 meaningful transformations to both datasets, including categorization by modifying existing columns\n",
    "\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load extracted data\n",
    "raw_df = pd.read_csv('data/raw_data.csv')\n",
    "incremental_df = pd.read_csv('data/incremental_data.csv')\n",
    "\n",
    "# Add a 'Purpose' column with sample values if it doesn't exist\n",
    "if 'Purpose' not in raw_df.columns:\n",
    "    raw_df['Purpose'] = np.random.choice(['Tourism', 'Business', 'Study', 'Other'], size=len(raw_df))\n",
    "if 'Purpose' not in incremental_df.columns:\n",
    "    incremental_df['Purpose'] = np.random.choice(['Business', 'Tourism', 'Other', 'Study'], size=len(incremental_df))\n",
    "\n",
    "# Fix missing values in both dataframes before transformation\n",
    "for df in [raw_df, incremental_df]:\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'O':  # Object type (string)\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "            mean_val = df[col].mean()\n",
    "            if pd.notnull(mean_val):\n",
    "                mean_val = float(f'{mean_val:.1g}')\n",
    "            df[col] = df[col].fillna(mean_val)  # Fill numeric with rounded mean (1 sig fig)\n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            df[col] = df[col].fillna(pd.Timestamp('1970-01-01'))\n",
    "        else:\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "\n",
    "def reorder_columns(df):\n",
    "    cols = list(df.columns)\n",
    "    # Move 'Purpose' after 'product'\n",
    "    if 'Purpose' in cols and 'product' in cols:\n",
    "        cols.remove('Purpose')\n",
    "        prod_idx = cols.index('product')\n",
    "        cols.insert(prod_idx + 1, 'Purpose')\n",
    "    # Move 'total_price' after 'unit_price'\n",
    "    if 'total_price' in cols and 'unit_price' in cols:\n",
    "        cols.remove('total_price')\n",
    "        unit_idx = cols.index('unit_price')\n",
    "        cols.insert(unit_idx + 1, 'total_price')\n",
    "    return df[cols]\n",
    "\n",
    "def transform(df):\n",
    "    # 1. Remove duplicate rows\n",
    "    df = df.drop_duplicates().copy()\n",
    "    \n",
    "    # 2. Handle missing values for all columns\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'O':  # Object type (string)\n",
    "            df.loc[:, col] = df[col].fillna('Unknown')\n",
    "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "            mean_val = df[col].mean()\n",
    "            if pd.notnull(mean_val):\n",
    "                mean_val = float(f'{mean_val:.1g}')\n",
    "            df.loc[:, col] = df[col].fillna(mean_val)  # Fill numeric with rounded mean (1 sig fig)\n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            df.loc[:, col] = df[col].fillna(pd.Timestamp('1970-01-01'))\n",
    "        else:\n",
    "            df.loc[:, col] = df[col].fillna('Unknown')\n",
    "    \n",
    "    # 3. Convert date columns to datetime (example: 'Date' or 'date')\n",
    "    for col in df.columns:\n",
    "        if 'date' in col.lower():\n",
    "            df.loc[:, col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            df.loc[:, col] = df[col].fillna(pd.Timestamp('1970-01-01'))\n",
    "    \n",
    "    # 4. Categorize 'Purpose' into broader groups (modifies existing column)\n",
    "    if 'Purpose' in df.columns:\n",
    "        df.loc[:, 'Purpose'] = df['Purpose'].replace({\n",
    "            'Tourism': 'Leisure',\n",
    "            'Business': 'Work',\n",
    "            'Study': 'Education'\n",
    "        })\n",
    "        # Any other value becomes 'Other'\n",
    "        df.loc[:, 'Purpose'] = df['Purpose'].where(df['Purpose'].isin(['Leisure', 'Work', 'Education']), 'Other')\n",
    "    \n",
    "    # 5. Remove time from 'order_date' if present\n",
    "    if 'order_date' in df.columns:\n",
    "        df.loc[:, 'order_date'] = pd.to_datetime(df['order_date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # 6. Enrichment: Add total_price = quantity * unit_price\n",
    "    if 'quantity' in df.columns and 'unit_price' in df.columns:\n",
    "        df.loc[:, 'total_price'] = df['quantity'] * df['unit_price']\n",
    "    \n",
    "    # 7. Reorder columns for output and CSV\n",
    "    df = reorder_columns(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply transformations\n",
    "transformed_full = transform(raw_df)\n",
    "transformed_incremental = transform(incremental_df)\n",
    "\n",
    "# Save transformed files\n",
    "os.makedirs('transformed', exist_ok=True)\n",
    "transformed_full.to_csv('transformed/transformed_full.csv', index=False)\n",
    "transformed_incremental.to_csv('transformed/transformed_incremental.csv', index=False)\n",
    "print(\"\\nTransformed files saved to 'transformed/transformed_full.csv' and 'transformed/transformed_incremental.csv'\")\n",
    "\n",
    "# Display only the head of the final outputs, with 'Purpose' after 'product' and 'total_price' after 'unit_price'\n",
    "def print_head_with_purpose(df, name):\n",
    "    df = reorder_columns(df)\n",
    "    print(f\"\\n=== Head of {name} ===\")\n",
    "    print(tabulate(df.head(), headers='keys', tablefmt='grid'))\n",
    "\n",
    "print_head_with_purpose(transformed_full, 'transformed_full.csv')\n",
    "print_head_with_purpose(transformed_incremental, 'transformed_incremental.csv')\n",
    "\n",
    "# Show summary of transformations for verification\n",
    "def transformation_summary(df, name):\n",
    "    summary = [\n",
    "        ['File', name],\n",
    "        ['Rows', len(df)],\n",
    "        ['Duplicate Rows', df.duplicated().sum()],\n",
    "        ['Any Missing Values', df.isnull().any().any()],\n",
    "    ]\n",
    "    if 'Purpose' in df.columns:\n",
    "        summary.append(['Purpose Unique Values', ', '.join(df['Purpose'].unique().astype(str))])\n",
    "    print(tabulate(summary, tablefmt='grid'))\n",
    "\n",
    "print(\"\\n=== Transformation Summary: transformed_full.csv ===\")\n",
    "transformation_summary(transformed_full, 'transformed_full.csv')\n",
    "print(\"\\n=== Transformation Summary: transformed_incremental.csv ===\")\n",
    "transformation_summary(transformed_incremental, 'transformed_incremental.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
